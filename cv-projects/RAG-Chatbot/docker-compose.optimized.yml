version: '3.8'

services:
  rag-chatbot-optimized:
    build:
      context: .
      dockerfile: Dockerfile.optimized
    ports:
      - "8501:8501"
      - "11434:11434"  # Ollama port
    environment:
      - OLLAMA_MODEL=llama3.2:3b
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
    volumes:
      - ./faiss_index:/app/faiss_index
      - ollama_data:/root/.ollama  # Persist Ollama models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s  # Reduced since model is pre-downloaded
    deploy:
      resources:
        limits:
          memory: 4G  # Recommended for llama3.2:3b
        reservations:
          memory: 2G

volumes:
  ollama_data:
